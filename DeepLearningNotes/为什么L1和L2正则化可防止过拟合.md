线性模型常用来处理回归和分类任务，为了防止模型处于过拟合状态，需要用L1正则化和L2正则化降低模型的复杂度，很多线性回归模型正则化的文章会提到L1是通过稀疏参数（减少参数的数量）来降低复杂度，L2是通过减小参数值的大小来降低复杂度。网上关于L1和L2正则化降低复杂度的解释五花八门，易让人混淆，看完各种版本的解释后过几天又全部忘记了。因此，文章的内容总结了网上各种版本的解释，并加上了自己的理解，希望对大家有所帮助。

### 目录
1、优化角度分析
2、梯度角度分析
3、先验概率角度分析
4、知乎点赞最多的图形角度分析
5、限制条件角度分析
6、PRML的图形角度分析
7、总结
### 1. 优化角度分析
##### 1）、L2正则化的优化角度分析
![001]()
在限定的区域，找到使E_{D}(w)最小的值。
图形表示为：