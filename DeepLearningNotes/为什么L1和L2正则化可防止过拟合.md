线性模型常用来处理回归和分类任务，为了防止模型处于过拟合状态，需要用L1正则化和L2正则化降低模型的复杂度，很多线性回归模型正则化的文章会提到L1是通过稀疏参数（减少参数的数量）来降低复杂度，L2是通过减小参数值的大小来降低复杂度。网上关于L1和L2正则化降低复杂度的解释五花八门，易让人混淆，看完各种版本的解释后过几天又全部忘记了。因此，文章的内容总结了网上各种版本的解释，并加上了自己的理解，希望对大家有所帮助。

### 目录
1、优化角度分析
2、梯度角度分析
3、先验概率角度分析
4、知乎点赞最多的图形角度分析
5、限制条件角度分析
6、PRML的图形角度分析
7、总结
### 1. 优化角度分析
##### 1）、L2正则化的优化角度分析
![001](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008001.png)
在限定的区域，找到使$E_{D}(w)$最小的值。
图形表示为：

![002](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008002.png)

![003](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008003.png)

![004](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008004.png)

![005](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008005.png)

![006](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008006.png)

![007](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008007.png)

![008](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008008.png)

![010](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008010.png)

![011](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008011.png)

![012](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008012.png)
![013](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008013.png)
![014](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008014.png)
![015](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008015.png)
![016](https://github.com/liuchuanloong/AI-Notes/blob/master/picture/pic20191008/pic20191008016.png)
